# Awesome Egocentric Vision [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
A curated list of egocentric vision resources, inspired by [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision) and [awesome-action-recognition](https://github.com/jinwchoi/awesome-action-recognition).

## Contents
- [Papers](#papers)
- [Datasets]
- [Blogs]
- [TODOs]

## Papers

* [Predicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition](https://arxiv.org/pdf/1803.09125) - Huang, Y., Cai, M., Li, Z., & Sato, Y., ECCV 2018 [[code]](https://github.com/hyf015/egocentric-gaze-prediction)

* [In the eye of beholder: Joint learning of gaze and actions in first person video](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yin_Li_In_the_Eye_ECCV_2018_paper.pdf) - Li, Y., Liu, M., & Rehg, J. M., ECCV 2018

* [Mitigating Bystander Privacy Concerns in Egocentric Activity Recognition with Deep Learning and Intentional Image Degradation](http://users.ece.utexas.edu/~ethomaz/papers/j2.pdf) - Dimiccoli, M., Marín, J., & Thomaz, E., Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2018

* [Summarization and Classification of Wearable Camera Streams by Learning the Distributions over Deep Features of Out-of-Sample Image Sequences](https://ieeexplore.ieee.org/document/8237726) - Perina, A., Mohammadi, S., Jojic, N., & Murino, V., ICCV 2017

* [Sr- clustering: Semantic regularized clustering for egocentric photo streams segmentation](https://arxiv.org/pdf/1512.07143) - Dimiccoli, M., Bolaños, M., Talavera, E., Aghaei, M., Nikolov, S. G., & Radeva, P., Computer Vision and Image Understanding 2017

* [Deep future gaze: Gaze anticipation on egocentric videos using adversarial networks](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf) - Zhang, M., Teck Ma, K., Hwee Lim, J., Zhao, Q., & Feng, J., CVPR 2017

* [Toward storytelling from visual lifelogging: An overview](https://arxiv.org/pdf/1507.06120.pdf) - Bolanos, M., Dimiccoli, M., & Radeva, P., IEEE Transactions on Human-Machine Systems 2017

* [Recognizing personal locations from egocentric videos](https://ieeexplore.ieee.org/document/7588113) - Furnari, A., Farinella, G. M., & Battiato, S., IEEE Transactions on Human-Machine Systems 2017

* [Automated capture and delivery of assistive task guidance with an eyewear computer: the GlaciAR system](https://arxiv.org/pdf/1701.02586) - Leelasawassuk, Teesid, Dima Damen, and Walterio Mayol-Cuevas, Augmented Human International Conference, ACM 2017

* [Privacy-Preserving Human Activity Recognition from Extreme Low Resolution](https://arxiv.org/pdf/1604.03196) - Ryoo, M. S., Rothrock, B., Fleming, C., & Yang, H. J., AAAI 2017

* [Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams](https://www.sciencedirect.com/science/article/abs/pii/S1077314216000679) - Aghaei, M., Dimiccoli, M., & Radeva, P., Computer Vision and Image Understanding 2016

* [Mining discriminative states of hands and objects to recognize egocentric actions with a wearable RGBD camera](http://cvrc.ece.utexas.edu/Publications/ShaohuaWanCVPRW2015_1_.pdf) - Wan, S., & Aggarwal, J. K., CVPRW 2015

* [Delving into egocentric actions](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Delving_Into_Egocentric_2015_CVPR_paper.pdf) - Li, Y., Ye, Z., & Rehg, J. M., CVPR 2015

* [Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions](http://homes.sice.indiana.edu/sbambach/papers/iccv-egohands.pdf) - Bambach, S., Lee, S., Crandall, D. J., & Yu, C., ICCV 2015

* [Generating Notifications for Missing Actions: Don’t forget to turn the lights off!](https://homes.cs.washington.edu/~ali/alarm-iccv.pdf) - Soran, Bilge, Ali Farhadi, and Linda Shapiro, ICCV 2015

* [You-Do, I-Learn: Discovering Task Relevant Objects and their Modes
of Interaction from Multi-User Egocentric Video](https://dimadamen.github.io/You-Do-I-Learn/Damen_BMVC2014.pdf) - Damen, D., Leelasawassuk, T., Haines, O., Calway, A., & Mayol-Cuevas, W. W., BMVC 2014 [[project web]](http://www.bmva.org/bmvc/2014/papers/paper059/index.html)

* [Temporal segmentation of egocentric videos](https://www.cse.iitd.ac.in/~chetan/papers/egocentric-cvpr14.pdf) - Poleg, Y., Arora, C., & Peleg, S., CVPR 2014

* [Story-Driven Summarization for Egocentric Video](https://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf) - Zheng Lu and Kristen Grauman, CVPR 2013 [[project web]](http://vision.cs.utexas.edu/projects/egocentric/storydriven.html)

* [Learning to predict gaze in egocentric video](http://ai.stanford.edu/~alireza/publication/Li-Fathi-Rehg-ICCV13.pdf) - Li, Yin, Alireza Fathi, and James M. Rehg, ICCV 2013

* [Discovering Important People and Objects for Egocentric Video Summarization](http://vision.cs.utexas.edu/projects/egocentric/egocentric_cvpr2012.pdf) - Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman, CVPR 2012

* [Social interactions: A first-person perspective](http://www.cs.utexas.edu/~cv-fall2012/slides/jake-expt.pdf) - Fathi, A., Hodgins, J. K., & Rehg, J. M., CVPR 2012

* [Learning to recognize daily actions using gaze](http://ai.stanford.edu/~alireza/publication/ECCV12.pdf) - Fathi, A., Li, Y., & Rehg, J. M, ECCV 2012

* [First-person vision](https://ieeexplore.ieee.org/document/6232429) - Takeo Kanade and Martial Herbert, Proceedings of the IEEE 100.8 2012

* [Detecting activities of daily living in first- person camera views](https://www.cs.cmu.edu/~deva/papers/ADL_2012.pdf) - Pirsiavash, H., & Ramanan, D., CVPR 2012

* [Learning to recognize objects in egocentric activities](https://homes.cs.washington.edu/~xren/publication/fathi_cvpr11_egocentric_objects.pdf) - Fathi, A., Ren, X., & Rehg, J. M., CVPR 2011

* [Fast unsupervised ego-action learning for first-person sports videos](http://www.dgcv.nii.ac.jp/Publications/Papers/2011/CVPR2011a.pdf) - Kitani, K. M., Okabe, T., Sato, Y., & Sugimoto, A., CVPR 2011 [[project web]](https://www.ri.cmu.edu/publications/fast-unsupervised-ego-action-learning-for-first-person-sports-videos/)

* [Figure-ground segmentation improves handled object recognition in egocentric video](https://ieeexplore.ieee.org/document/5540074) - Ren, Xiaofeng, and Chunhui Gu, CVPR 2010

* [Temporal segmentation and activity classification from first-person sensing](https://ieeexplore.ieee.org/document/5204354) - Spriggs, Ekaterina H., Fernando De La Torre, and Martial Hebert, Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009

* [Eye movements and the control of actions in everyday life](https://www.sciencedirect.com/science/article/pii/S1350946206000036) - Land, Michael F., Progress in retinal and eye research 2006

* [Wearable hand activity recognition for event summarization](https://ieeexplore.ieee.org/document/1550796) - Mayol, W. W., & Murray, D. W., IEEE International Symposium on Wearable Computers, 2005

* [Context-based vision system for place and object recognition](https://www.cs.ubc.ca/~murphyk/Papers/iccv03.pdf) - Torralba, A., Murphy, K. P., Freeman, W. T., & Rubin, M. A., ICCV 2003. [[project web]](https://www.cs.ubc.ca/~murphyk/Vision/placeRecognition.html)

* [Real-time localization and mapping with wearable active vision](https://ieeexplore.ieee.org/document/1240684) - Davison, A. J., Mayol, W. W., & Murray, D. W., The Second IEEE and ACM International Symposium 2003

* [Wearable computing: a first step toward personal imaging](https://ieeexplore.ieee.org/document/566147) - Steve Mann, Computer, vol. 30 1997.

## Datasets

| Dataset | URL | Settings | Annotations | Tasks |
| ------- | :--:| :------: | :---------: | ----: |
| EPIC-Kitchens 2018 | https://epic-kitchens.github.io/2018 | 32 subjects performing unscripted actions in their native environments | action segments, object annotations | Action recognition, Action Anticipation, Object Detection |
| Charade-Ego | https://allenai.org/plato/charades/ | paired first-third person videos | action classes | Action recognition |
| EGTEA Gaze+ | http://ai.stanford.edu/~alireza/GTEA/ | 32 subjects, 86 sessions, 28 hours | action segments, gaze, hand masks | Understading daily activities, action recognition | 
| ADL | https://www.csee.umbc.edu/~hpirsiav/papers/ADLdataset/ | 20 subjects performing daily activities in their native environments | activity segments, objects | Detecting activities of daily living | 
| CMU kitchen | http://www.cs.cmu.edu/~espriggs/cmu-mmac/annotations/ | multimodal, 18 subjects cooking 5 different recipes: brownies, eggs, pizza, salad, sandwich | action segments | Understading daily activities |
| EgoSeg | http://www.vision.huji.ac.il/egoseg/ | Long term actions (walking, running, driving, etc.) | long term activity | Temporal Segmentation, Indexing | 
| First-Person Social Interactions | http://ai.stanford.edu/~alireza/Disney/ | 8 subjects at disneyworld | Activities: walking, waiting, gathering, sitting, buying something, eating, etc. | Recognizing social interactions |
| UEC Dataset | http://www.cs.cmu.edu/~kkitani/datasets/ | two choreographed datasets with different egoactions (walk, jump, climb, etc.) + 6 youtube sports videos | activities | Unsupervised activity recognition |
| JPL | http://michaelryoo.com/jpl-interaction.html | interaction with a robot | activities performed on the robot + pose | Interaction recognition/prediction |
| Multimodal Egocentric Activity Dataset | https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/ | 15 seconds clips of 20 activities | activity (walking, elevator, etc.) | Life-logging |
| FPPA | http://tamaraberg.com/prediction/Prediction.html | Five subjects performing 5 daily actions | activity (drinking water, putting on clothes, etc.) | Temporal prediction |
| UT Egocentric | http://vision.cs.utexas.edu/projects/egocentric/index.html | 3-5 hours long videos capturing a person's day | important regions | Summarization |
| VINST/ Visual Diaries | http://www.csc.kth.se/cvap/vinst/NovEgoMotion.html | 31 videos capturing the visual experience of a subject walkin from metro station to work | location id, novel egomotion | Novelty detection |
| Bristol Egocentric Object Interaction (BEOID) | https://www.cs.bris.ac.uk/~damen/BEOID/ | 8 subjects, six locations. Interaction with objects and environment |  gaze, objects, mode of interaction (pick, plug, etc.) | Provide assistance on object usage |
| Object Search Dataset | https://github.com/Mengmi/deepfuturegaze_gan | 57 sequences of 55 subjects on search and retrieval tasks | gaze | gaze prediction |
| UNICT-VEDI | http://iplab.dmi.unict.it/VEDI/ | different subjects visiting a museum | location, observed objects | localizing visitors of a museum and estimating their attention | 
| UNICT-VEDI-POI | http://iplab.dmi.unict.it/VEDI_POIs/ | different subjects visiting a museum | object bounding boxes annotations, observed objects | recognizing points of interest observed by the visitors | 
| Simulated Egocentric Navigations | http://iplab.dmi.unict.it/SimulatedEgocentricNavigations/ | simulated navigations of a virtual agent within a large building | 3-DOF pose of the agent in each image | egocentric localization | 
| EgoCart | http://iplab.dmi.unict.it/EgocentricShoppingCartLocalization/ | egocentric images collected by a shopping cart in a retail store | 3-DOF pose of the shopping cart in each image | egocentric localization | 
| Unsupervised Segmentation of Daily Livign Activities | http://iplab.dmi.unict.it/dailylivingactivities | egocentric videos of daily activities | activities | unsupervised segmentation with respect to the activities | 
| Visual Market Basket Analysis | http://iplab.dmi.unict.it/vmba/ | egocentric images colelcted by a shopping cart in a retail store | class-location of each image | egocentric localization | 
| Location Based Segmentation of Egocentric Videos | http://iplab.dmi.unict.it/PersonalLocationSegmentation/ | egocentric videos of daily activities | location classes | egocentric localization, video indexing | 
| Recognition of Personal Locations from Egocentric Videos | http://iplab.dmi.unict.it/PersonalLocations/ | egocentric videos clips of daily | activities | location classes | recognizing personal locations | 
| EgoGesture | http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html | 2k videos from 50 subjects performing 83 gestures | Gesture labels, depth | Gesture recognition | 
| EgoHands | http://vision.soic.indiana.edu/projects/egohands/ | 48 videos of interactions between two people | Hand segmentation masks |Egocentric hand segmentation |
| DoMSEV | http://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/ | 80 hours/different activities | Scene/Action labels with IMU, GPS mad depth | Summarization | 
| DR(eye)VE | http://aimagelab.ing.unimore.it/dreyeve | 74 videos of people driving | Eye fixations | Autonomous and assisted driving |
| THU-READ | http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php | 8 subjects performing 40 actions with a head- mounted RGBD camera | Action segments | RGBD egocentric action recognition |
<!-- | EGO-HPE | http://imagelab.ing.unimore.it/imagelab2015/researchactivity.asp?idAttivita=23 | Egocentric videos for head pose estimation | Head pose of the subjects | Head-pose estimation | 
| EGO-GROUP | http://imagelab.ing.unimore.it/imagelab2015/researchactivity.asp?idAttivita=23 | 18 videos of people engaging social relationships | Social relationships | Understanding social relationships |  -->
<!-- | LENA: An egocentric video database of visual lifelog | http://people.sutd.edu.sg/~1000892/dataset | 13 activities performed by 10 subjects(Google Glass) | activity (walking, elevator, etc.) | Life-logging | -->

## Blogs

## TODOs
